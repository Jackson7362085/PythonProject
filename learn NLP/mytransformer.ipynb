{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from  tensorflow import keras\n",
    "\n",
    "import utils\n",
    "\n",
    "Drop_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Multihead(keras.layers.Layer):\n",
    "    def __init__(self,model_dim,n_head,Drop_rate):\n",
    "        super(Multihead, self).__init__()\n",
    "        self.model_dim=model_dim\n",
    "        self.n_head=n_head\n",
    "        self.head_dim=self.model_dim//self.n_head\n",
    "        self.wq=keras.layers.Dense(self.n_head*self.head_dim)\n",
    "        self.wk=keras.layers.Dense(self.n_head*self.head_dim)\n",
    "        self.wv=keras.layers.Dense(self.n_head*self.head_dim)\n",
    "        self.dense=keras.layers.Dense(self.model_dim)\n",
    "        self.drop=keras.layers.Dropout(rate=Drop_rate)\n",
    "    def call(self,q,k,v,mask,training):\n",
    "        o=self.scale_dot(q,k,v,mask)\n",
    "        o=self.dense(o)\n",
    "        return self.drop(o,training=training)\n",
    "    def scale_dot(self,q,k,v,mask):\n",
    "        num_k=tf.cast(k.shape[-1],tf.float32)\n",
    "        _q=self.wq(q)\n",
    "        _k=self.wk(k)\n",
    "        _v=self.wv(v)\n",
    "        _q=self.reshape(_q)\n",
    "        _k=self.reshape(_k)\n",
    "        _v=self.reshape(_v)\n",
    "        score=tf.matmul(_q,_k,transpose_b=True)/tf.sqrt(num_k+1e-8) #n,head,step,step\n",
    "        if mask is not None:\n",
    "            score+=mask*-1e9\n",
    "        attention=tf.nn.softmax(score,axis=-1)\n",
    "        context=tf.matmul(attention,_v) #n head step dim\n",
    "        context=tf.transpose(context,perm=[0,2,1,3])\n",
    "        return tf.reshape(context,(context.shape[0],context.shape[1],-1))\n",
    "    def reshape(self,x):\n",
    "        x=tf.reshape(x,(x.shape[0],x.shape[1],self.n_head,self.head_dim))\n",
    "        return tf.transpose(x,perm=[0,2,1,3])\n",
    "\n",
    "class PositionWiseFFN(keras.layers.Layer):\n",
    "    def __init__(self,model_dim):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.dense=keras.layers.Dense(model_dim*4)\n",
    "        self.dense1=keras.layers.Dense(model_dim)\n",
    "    def call(self,x):\n",
    "        o=self.dense(x,activation=keras.activations.relu)\n",
    "        return self.dense1(o)\n",
    "\n",
    "class EncoderLayer(keras.layers.Layer):\n",
    "    def __init__(self,model_dim,n_head,Drop_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mul=Multihead(model_dim,n_head,Drop_rate)\n",
    "        self.ffn=PositionWiseFFN(model_dim)\n",
    "        self.drop=keras.layers.Dropout(rate=Drop_rate)\n",
    "        self.l=[keras.layers.LayerNormalization(axis=-1) for _ in range(2)]\n",
    "\n",
    "    def call(self,x,mask,training):\n",
    "        o1=self.mul.call(x,x,x,mask,training)\n",
    "        o1=self.l[0](o1+x)\n",
    "        o2=self.drop(self.ffn(o1),training=training)\n",
    "        return self.l[1](o1+o2)\n",
    "\n",
    "class Encoder(keras.layers.Layer):\n",
    "    def __init__(self,n_layer,model_dim,n_head,Drop_rate):\n",
    "        self.n_layer=n_layer\n",
    "        super(Encoder, self).__init__()\n",
    "        self.l=[EncoderLayer(model_dim,n_head,Drop_rate) for _ in range(n_layer)]\n",
    "    def call(self, x,mask,training):\n",
    "        for i in range(self.n_layer):\n",
    "            x=self.l[i].call(x,mask,training)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(keras.layers.Layer):\n",
    "    def __init__(self,model_dim,n_head,Drop_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mh=[Multihead(model_dim,n_head,Drop_rate) for _ in range(2)]\n",
    "        self.ln=[keras.layers.LayerNormalization(axis=-1) for _ in range(3)]\n",
    "        self.ffn=PositionWiseFFN(model_dim)\n",
    "        self.drop=keras.layers.Dropout(Drop_rate)\n",
    "    def call(self,x,y,xz_pad_mask,yz_look_ahead_mask,training):\n",
    "        o1=self.mh[0].call(y,y,y,yz_look_ahead_mask,training)\n",
    "        o2=self.ln[0](o1+y)\n",
    "        o3=self.mh[1].call(o2,x,x,xz_pad_mask)\n",
    "        o4=self.ln[1](o3+o2)\n",
    "        o5=self.drop(self.ffn(o4))\n",
    "        o=self.ln[2](o5+o4)\n",
    "        return o\n",
    "\n",
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self,model_dim,n_head,Drop_rate,n_layer):\n",
    "        self.n_layer=n_layer\n",
    "        super(Decoder, self).__init__()\n",
    "        self.l=[DecoderLayer(model_dim,n_head,Drop_rate) for _ in range(n_layer)]\n",
    "\n",
    "    def call(self,x,y,xz_pad_mask,yz_look_ahead_mask,training):\n",
    "        for i in range(self.n_layer):\n",
    "            y=self.l[i].call(x,y,xz_pad_mask,yz_look_ahead_mask,training)\n",
    "        return y\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class PositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self,max_len,model_dim,n_vocab):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.max_len=max_len\n",
    "        self.model_dim=model_dim\n",
    "        self.emb=keras.layers.Embedding(n_vocab,model_dim)\n",
    "    def position(self):\n",
    "        pos=np.arange(self.max_len)[:,None]\n",
    "        q=[np.power(10000,2*i) for i in range(self.model_dim)]\n",
    "        q=q[None,:]\n",
    "        matrix=pos/q\n",
    "        matrix[:,0::2]=np.sin(matrix[:,0::2])\n",
    "        matrix[:,1::2]=np.cos(matrix[:,1::2])\n",
    "        matrix=tf.constant(matrix[None,:,:])\n",
    "        return matrix\n",
    "    def call(self,x):\n",
    "        return self.position()+self.emb(x)\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "\n",
    "    def __init__(self,n_head,model_dim,Drop_rate,n_layer,n_vocab,max_len,padding_idx):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.padding_idx=padding_idx\n",
    "        self.max_len=max_len\n",
    "        self.encoder=Encoder(n_layer,model_dim,n_head,Drop_rate)\n",
    "        self.decoder=Decoder(model_dim,n_head,Drop_rate,n_layer)\n",
    "        self.emb=PositionEmbedding(max_len,model_dim,n_vocab)\n",
    "        self.o=keras.layers.Dense(n_vocab)\n",
    "        self.loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "        self.opt=keras.optimizers.Adam(0.001)\n",
    "\n",
    "    def call(self,x,y,training):\n",
    "        emb_x=self.emb(x)\n",
    "        emb_y=self.emb(y)\n",
    "        pad_mask=self.pad_mask(x)\n",
    "        xz=self.encoder.call(emb_x,pad_mask,training)\n",
    "        yz=self.decoder.call(xz,emb_y,pad_mask,self.look_ahead_mask(y),training)\n",
    "        return self.o(yz)\n",
    "\n",
    "    def step(self,x,y,training):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logit=self.call(x,y[:,:-1],training)\n",
    "            bool_mask=tf.math.not_equal(y,self.padding_idx)\n",
    "            loss=tf.reduce_mean(tf.boolean_mask(tf.losses(logit,y[:,1:]),bool_mask))\n",
    "        grads=tape.gradient(loss,self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads,self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def pad_mask(self,seq):\n",
    "        mask=tf.math.equal(seq,self.padding_idx)\n",
    "        mask=tf.cast(mask,tf.float32)\n",
    "        return mask[:,None,None,:]\n",
    "\n",
    "    def look_ahead_mask(self,seq):\n",
    "        m=1-tf.linalg.band_part(tf.ones(self.max_len,self.max_len),-1,0)\n",
    "        judge=tf.math.equal(seq,self.padding_idx)\n",
    "        mask=tf.where(judge[:,None,None,:],1,m[None,None,:,:])\n",
    "        return mask\n",
    "\n",
    "    def reference(self,x,v2i,i2v):\n",
    "        y=[[v2i['<GO>']] for _ in range(len(x))]\n",
    "        y=utils.pad_zero(y,self.max_len)\n",
    "        x=utils.pad_zero(x,self.max_len)\n",
    "        idx=0\n",
    "        emb_x=self.emb(x)\n",
    "        xz=self.encoder.call(emb_x,self.pad_mask(x),training=False)\n",
    "        while True:\n",
    "            yz=self.decoder.call(xz,self.emb(y),self.pad_mask(x),self.look_ahead_mask(y))\n",
    "            index=np.argmax(yz,axis=1)\n",
    "            y[:,idx]=index\n",
    "            idx+=1\n",
    "            if idx==self.max_len:\n",
    "                break\n",
    "        return [\"\".join([i2v[item] for item in y[j,1:]]) for j in range(self.max_len)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}